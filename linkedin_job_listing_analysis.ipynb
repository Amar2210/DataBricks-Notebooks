{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2234954e-2e23-48e9-b54c-ed7fe35e2d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.parquet(\n",
    "    \"/Volumes/workspace/default/raw_job_postings/0000.parquet\"\n",
    ")\n",
    "\n",
    "# print(df_raw.count())\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbc0bc27-13f9-4b2b-8e1c-caab2c29c1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500e030b-028c-4413-bd27-ec0f91d6ce62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_core = df_raw.select(\n",
    "    col(\"job_id\"),\n",
    "    col(\"title\"),\n",
    "    col(\"description\"),\n",
    "    col(\"skills_desc\")\n",
    ")\n",
    "\n",
    "# df_core.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd5ec88c-aee2-4719-aebd-48508c43c30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "df_core.select(\n",
    "    spark_sum(col(\"job_id\").isNull().cast(\"int\")).alias(\"job_id_null_count\"),\n",
    "    spark_sum(col(\"title\").isNull().cast(\"int\")).alias(\"title_null_count\"),\n",
    "    spark_sum(col(\"description\").isNull().cast(\"int\")).alias(\"description_count\"),\n",
    "    spark_sum(col(\"skills_desc\").isNull().cast(\"int\")).alias(\"skills_desc_count\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908b64f5-b670-42f4-ad9c-d4d7da6b7cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_non_null = df_core.filter(\n",
    "    col(\"job_id\").isNotNull() &\n",
    "    col(\"title\").isNotNull() &\n",
    "    col(\"description\").isNotNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59b90372-9bc3-4a0a-9437-1a082c94aa32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Before:\", df_core.count())\n",
    "print(\"After:\", df_non_null.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22b50d4c-4208-497b-b244-bb35647f90d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "duplicate_groups = (\n",
    "    df_non_null\n",
    "    .groupBy(\"title\", \"description\")\n",
    "    .count()\n",
    "    .filter(col(\"count\")>1)\n",
    "    .count()\n",
    ")\n",
    "duplicate_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccf90dd6-591f-439b-ad5c-76a3833451d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dedup = df_non_null.dropDuplicates([\"title\", \"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb3ef128-6f07-48bf-9112-0d4cd3448ed9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"After Dedup count:\", df_dedup.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcc3a198-f5aa-4861-8506-c888e72c82b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dedup.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17eed5ea-7874-4e27-b9e2-eaea12133fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Role Filter for Data Engineer\n",
    "from pyspark.sql.functions import lower\n",
    "\n",
    "df_de = df_dedup.filter(\n",
    "    lower(col(\"title\")).rlike(\n",
    "        \"data engineer|analytics engineer|big data engineer|data platform engineer\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb03f405-feb1-4ff1-b80d-724c9603776c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_de.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7910354d-23df-4f95-b059-99b9e544a232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_de.select(\"title\").distinct().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27a4f3fa-a3da-4b93-95d4-9ddb0edc29fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_de.select(\"description\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f5cc049-4a62-42b3-8647-8a282d208493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_de = df_de.select(\n",
    "    \"job_id\",\n",
    "    \"title\",\n",
    "    \"description\",\n",
    "    \"skills_desc\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a83d82-0de5-43c8-b1c3-3de864ba611e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df_text = df_de.withColumn(\n",
    "    \"clean_text\",\n",
    "    lower(col(\"description\"))\n",
    ")\n",
    "\n",
    "df_text = df_text.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(\"clean_text\", r\"http\\S+|www\\S+\", \" \")\n",
    ")\n",
    "\n",
    "df_text = df_text.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(\"clean_text\", r\"[^a-z0-9+\\.\\# ]\", \" \") #replace anything that is not one of these with a space\n",
    ")\n",
    "\n",
    "df_text = df_text.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(\"clean_text\", r\"\\s+\", \" \") #Normalize whitespaces, it removes all the extra spaces\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfe5849-fe1a-4ac3-9a78-07a04a844441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_text.select(\"clean_text\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e0e919-ec5c-46f0-a4c0-3860dbda0cee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "df_text.select(\n",
    "    length(\"clean_text\").alias(\"text_length\")\n",
    ").summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08745b74-9646-47a5-9a81-a9c425c2ec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_nlp = df_text.select(\n",
    "    \"job_id\",\n",
    "    \"title\",\n",
    "    \"clean_text\",\n",
    "    \"description\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641fdd60-0a3d-475c-8126-707b1032ef51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_engineer_skills = [\n",
    "    # languages\n",
    "    \"python\", \"sql\", \"java\", \"scala\",\n",
    "\n",
    "    # big data\n",
    "    \"spark\", \"pyspark\", \"hadoop\", \"hive\", \"kafka\",\n",
    "\n",
    "    # cloud\n",
    "    \"aws\", \"azure\", \"gcp\", \"s3\", \"redshift\", \"bigquery\",\n",
    "\n",
    "    # orchestration / etl\n",
    "    \"airflow\", \"dbt\", \"etl\", \"elt\",\n",
    "\n",
    "    # databases\n",
    "    \"postgres\", \"mysql\", \"snowflake\", \"databricks\",\n",
    "\n",
    "    # formats / tools\n",
    "    \"parquet\", \"delta\", \"delta lake\",\n",
    "\n",
    "    # misc\n",
    "    \"ci/cd\", \"git\", \"docker\", \"kubernetes\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7fa668-1abc-46ed-a581-8fdc721300b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "skill_patterns = {\n",
    "    skill: re.compile(rf\"\\b{re.escape(skill)}\\b\")\n",
    "    for skill in data_engineer_skills\n",
    "} # Here we create a dictionary where key is skill and value is a compiled regex pattern to find exact match\n",
    "# So the value is NOT the skill string itself — it’s a precompiled search rule.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2084171-3a13-4e7f-8cd7-f4e0f3e588c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def extract_skills(text): #this function requires an argument\n",
    "    found = []\n",
    "    if text:\n",
    "        for skill, pattern in skill_patterns.items():\n",
    "            if pattern.search(text): #if pattern from the above dict matches the text input it will append to the list\n",
    "                found.append(skill)\n",
    "    return found\n",
    "\n",
    "extract_skills_udf = udf(extract_skills, ArrayType(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7009a67-765d-4bb9-aa36-0ca745086c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skills = df_nlp.withColumn(\n",
    "    \"extracted_skills\",\n",
    "    extract_skills_udf(\"clean_text\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "974958f4-f29a-4fea-b4e7-e22e767e8d33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skills.select(\"title\", \"extracted_skills\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6224db3d-5a46-4122-9b62-8af9e3da7d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, round\n",
    "\n",
    "skill_counts = (\n",
    "    df_skills\n",
    "    .select(explode(\"extracted_skills\").alias(\"skill\"))\n",
    "    .groupBy(\"skill\")\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12356833-063c-433a-95a6-d3a8170b12d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_counts.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "022e9745-c16f-4c86-9f13-f106d2da35c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total_jobs = df_skills.count()\n",
    "\n",
    "skill_stats = skill_counts.withColumn(\n",
    "    \"percentage\",\n",
    "    round((col(\"count\") / total_jobs) * 100,1)\n",
    ")\n",
    "\n",
    "skill_stats.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05443560-7d87-47aa-886d-9d7f25451427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_stats.orderBy(\"percentage\", ascending=False).show(15, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c6a2f4-2509-49d4-ba05-35da7506f9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Skill Normalization and Ontology\n",
    "### Right now the output treats apache spark, pyspark, spark as three different skills but we know that they all are same and so we need to count or treat them as one entity/capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09dfe221-54ad-46b4-a8e1-6522a209a717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ontology Disctionary\n",
    "skill_ontology = {\n",
    "    \"python\": [\"python\"],\n",
    "    \"sql\": [\"sql\", \"postgresql\", \"mysql\", \"sqlite\"],\n",
    "    \n",
    "    \"spark\": [\"spark\", \"pyspark\", \"apache spark\"],\n",
    "    \"hadoop\": [\"hadoop\", \"hdfs\"],\n",
    "    \n",
    "    \"aws\": [\"aws\", \"amazon web services\", \"s3\", \"ec2\", \"redshift\"],\n",
    "    \"azure\": [\"azure\", \"adf\", \"synapse\"],\n",
    "    \"gcp\": [\"gcp\", \"bigquery\", \"cloud composer\"],\n",
    "    \n",
    "    \"airflow\": [\"airflow\", \"apache airflow\"],\n",
    "    \"dbt\": [\"dbt\"],\n",
    "    \n",
    "    \"kafka\": [\"kafka\", \"apache kafka\"],\n",
    "    \n",
    "    \"databricks\": [\"databricks\"],\n",
    "    \"snowflake\": [\"snowflake\"],\n",
    "    \n",
    "    \"docker\": [\"docker\"],\n",
    "    \"kubernetes\": [\"kubernetes\", \"k8s\"],\n",
    "    \n",
    "    \"etl\": [\"etl\", \"elt\"],\n",
    "    \"delta lake\": [\"delta\", \"delta lake\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1338613a-ae7e-4b11-a8db-37740a1051de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "normalized_patterns = {\n",
    "    canonical: [\n",
    "        re.compile(rf\"\\b{re.escape(variant)}\\b\")\n",
    "        for variant in variants\n",
    "    ]\n",
    "    for canonical, variants in skill_ontology.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b71d4867-608e-4d03-b589-26a1fd6ea2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import ArrayType, StringType\n",
    "\n",
    "def extract_normalized_skills(text):\n",
    "    found_normalized_skills = []\n",
    "    if text:\n",
    "        for canonical_skill, patterns in normalized_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern.search(text):\n",
    "                    found_normalized_skills.append(canonical_skill)\n",
    "                    break  # stop once one variant matches\n",
    "    return found_normalized_skills\n",
    "\n",
    "extract_normalized_skills_udf = udf(\n",
    "    extract_normalized_skills,\n",
    "    ArrayType(StringType())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "458db097-6851-4f4d-8473-fd933fb7f500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_norm = df_nlp.withColumn(\n",
    "    \"normalized_skills\",\n",
    "    extract_normalized_skills_udf(\"clean_text\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f13a37-8aa4-4d1d-becc-a2ddd00690ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_norm.select(\"title\", \"normalized_skills\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a58ac37-b5a4-4044-a6b7-0c4ff808d89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col, round\n",
    "\n",
    "skill_counts_norm = (\n",
    "    df_norm\n",
    "    .select(explode(\"normalized_skills\").alias(\"skill\"))\n",
    "    .groupBy(\"skill\")\n",
    "    .count()\n",
    ")\n",
    "\n",
    "total_jobs = df_norm.count()\n",
    "\n",
    "skill_stats_norm = skill_counts_norm.withColumn(\n",
    "    \"percentage\",\n",
    "    round((col(\"count\") / total_jobs) * 100, 1)\n",
    ").orderBy(\"percentage\", ascending=False)\n",
    "\n",
    "skill_stats_norm.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23410ae1-8796-4044-b805-7232256903ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Implemented a skill ontology and normalization layer to consolidate variant skill mentions into canonical capabilities prior to downstream analytics. I have answered which skills are in demand for Data Engineers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "488523b8-5f7b-4fb5-be01-648d2f65a403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we try to answers questions like ----\n",
    "Which skills appear together?\n",
    "Are there any disticnt skill bundles?\n",
    "Are there any sub-roles inside Data Engineer?\n",
    "Co-occurence is important than frequency because frequency tells python is mentioned in 78% of the jobs, while co-occurence tells Python, SQL, AWS appear often. Thats the difference between popularity vs capability profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a6b2ad5-e600-49fa-a7c5-facd17de3959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skill_long = (\n",
    "    df_norm\n",
    "    .select(\n",
    "        col(\"job_id\"),\n",
    "        explode(\"normalized_skills\").alias(\"skills\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fc40a00-d36b-4e02-8391-e521d56707f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skill_long.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d622b1b-b14a-4665-860c-90be60f7acab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_job_counts = (\n",
    "    df_skill_long\n",
    "    .groupBy(\"skills\")\n",
    "    .agg({\"job_id\": \"count\"})\n",
    "    .withColumnRenamed(\"count(job_id)\", \"job_count\")\n",
    "    .orderBy(\"job_count\", ascending = False)\n",
    ")\n",
    "\n",
    "skill_job_counts.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fb55bb-d6f2-4e21-8f5d-f6c2407b8ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pairs = (\n",
    "    df_skill_long.alias(\"a\")\n",
    "    .join(\n",
    "        df_skill_long.alias(\"b\"),\n",
    "        on=\"job_id\"\n",
    "    )\n",
    "    .where(col(\"a.skills\") < col(\"b.skills\"))  # avoid duplicates & self-pairs\n",
    "    .select(\n",
    "        col(\"a.skills\").alias(\"skill_a\"),\n",
    "        col(\"b.skills\").alias(\"skill_b\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf5a191-6d3d-4bb3-aae7-31cfd86c3a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "co_occurrence = (\n",
    "    df_pairs\n",
    "    .groupBy(\"skill_a\", \"skill_b\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"intersection\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fe1842-4fb9-4b37-ba34-24ba5fabe2bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "co_occurrence.orderBy(\"intersection\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0a96447-aad5-46be-b664-42e87c37f947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jaccard = (\n",
    "    co_occurrence\n",
    "    .join(\n",
    "        skill_job_counts\n",
    "            .withColumnRenamed(\"skills\", \"skill_a\")\n",
    "            .withColumnRenamed(\"job_count\", \"count_a\"),\n",
    "        on=\"skill_a\"\n",
    "    )\n",
    "    .join(\n",
    "        skill_job_counts\n",
    "            .withColumnRenamed(\"skills\", \"skill_b\")\n",
    "            .withColumnRenamed(\"job_count\", \"count_b\"),\n",
    "        on=\"skill_b\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bed505f-9d37-4dfd-ad6b-d7937f3d82f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "jaccard = jaccard.withColumn(\n",
    "    \"jaccard_similarity\",\n",
    "    round(\n",
    "        col(\"intersection\") /\n",
    "        (col(\"count_a\") + col(\"count_b\") - col(\"intersection\")),\n",
    "        3\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023650a5-d14e-49ad-8843-b5e03a190074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jaccard.orderBy(\"jaccard_similarity\", ascending=False).show(15, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5631566275374587,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "linkedin_job_listing_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
